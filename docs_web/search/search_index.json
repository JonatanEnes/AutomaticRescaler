{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Serverless Containers is able to dynamically scale a container resource limit ( e.g., CPU, Memory, disk and network ) in order to adapt them to the real usage , at any moment and in real-time. For a brief summary of this tool you can visit its homepage website. In order to see data from real experiments where this tool was used, you can visit this demo . For the uncommented source code you can visit its GitHub . Serverless Containers has also been the subject of a publication in Future Generation Computer Systems (FGCS) , which is available online . ( preprint ) In this publication the framework is thoroughly described with all technichal detail, and several experiment examples are also presented. This documentation webpage gives a more detailed description of the framework but without delving into technichal details as in the available publication. The webpage has been structured with the following sections: Use case : This section summarizes the core use case of this framework with a simple example. Architecture : Which briefly describes the architecture and design used. Deployment : In this section it is described how to deploy the framework overall. Some guidelines are also provided. Source Code (external) : If you are interested on the low-level code and code documentation. Sponsors : Some comments on the backers and sponsors of this framework.","title":"Introduction"},{"location":"architecture/","text":"High-level diagram This framework has been designed using a microservice approach in order to ease its development as well as to create speciliazed units that can be reused or improved in isolation. In addition, by using this paradigm it is also possible to implement framework that inherently presents an internal parallelism that is useful when dealing with scenarios that require responsiveness, such as is the case with real-time and on-demand resource scaling. The next image shows a high-level diagram of the scenario on which the framework is deployed: [1] Beginning with the framework's inputs , there are two: 1) the actions, that control the framework's behavior, both performed by an user or by another program through the API; and, 2) the resource monitoring time series, currently provided by an external framework ( BDWatchdog ), that are used in the policy decision for the resource scaling operations. [2] Continuing with the actual serverlesss containers framework , which groups several microservices, some of which are placed on the controlled hosts. The framework's inner workings are further specified on the following sections. [3] And finishing with the controlled infrastructure , which usually consists of several hosts running each one several containers. Currently only the containers backed by the cgroups file system are supported by design and, more specifically, Linux Containers (LXC) have been used and thus have been proven to work with this framewok. Microservice architecture As previously stated, the design followed to create the architecture of this framework uses several microservices that communicate and exchange information. The following image shows a high-level diagram of the microservices layout: As it can be seen, the microservices can be separated into active and passive ones, with the difference being that the passive ones focus on feedback operations to keep the framework continuously updates on the infrastructure's state, and the active ones that use such information to change the state (the container's resource limits) accordingly as needed. Passive Microservices The passive microservices are needed to continuosly keep updated the central database (State Database) with all the metada that tracks the state of the infrastructure, from the number of containers and its thresholds to the allocated amounts of resources. Some passive microservices are used to create aggregated data for entities such as applications (i.e., representing a group of containers) or to persist temporary information into a persistent database. Structure Snapshoter : Continuously polls the limits of the containers and writes that information into the State Database. Database Snapshoter : Forwards the information temporarily stored on the State Database to a peristent database, thus creating time series. Refeeder Snapshoter : Aggregates and creates new metadata from existing one (e.g., allocated amount of CPU for an application composed of 3 containers) Active Microservices The active microservices are the actual ones that perform the scaling of the resources via changing the container resource limits on the underlying cgroups file system through a coordinated chain of events. As described on the \"Scaling policy\" subsection of the \"Use Case\" section, in order to perform a scaling operation, the resource usage has to exceed the upper, or drop below the lower limit (1). After a configured time amount has passed on this state, the Guardian microservice will create a scaling request (2) with the specific amount of resource to be changed. Such request will be picked up and processed by the Scaler (3) and applied accordingly (4). More specific functionalities of the microservices are described next: Guardian : Working with time windows, it matches the real resource usages of the containers, as provided with the external resource monitoring, with the container limits stored on the State Database. As a result of the matches it generates scaling requests. Scaler : Polls the State Database looking for requests to be applied to the containers. Other/Common Microservices and Databases Some microservices have an auxiliar role and are used both by active and passive microservices. Orchestrator : Exposes an API that can be used both by humans or scripts to configure the framework. Container Scaler : This microservice has to be deployed on every infrastructure node whose hosted containers are to be scaled. This service is able to read and write the crgoups file system to perform the actual resource limit scalings. More info If what is described on this documentation webpage does not answer all of your doubts regarding the technical details, or simply is not enough for you and you want the specifics, this framework has been published on a full-length paper on the FGCS journal. If you don't have access to the journal paper, you can still access the preprint on this link .","title":"Architecture"},{"location":"architecture/#high-level-diagram","text":"This framework has been designed using a microservice approach in order to ease its development as well as to create speciliazed units that can be reused or improved in isolation. In addition, by using this paradigm it is also possible to implement framework that inherently presents an internal parallelism that is useful when dealing with scenarios that require responsiveness, such as is the case with real-time and on-demand resource scaling. The next image shows a high-level diagram of the scenario on which the framework is deployed: [1] Beginning with the framework's inputs , there are two: 1) the actions, that control the framework's behavior, both performed by an user or by another program through the API; and, 2) the resource monitoring time series, currently provided by an external framework ( BDWatchdog ), that are used in the policy decision for the resource scaling operations. [2] Continuing with the actual serverlesss containers framework , which groups several microservices, some of which are placed on the controlled hosts. The framework's inner workings are further specified on the following sections. [3] And finishing with the controlled infrastructure , which usually consists of several hosts running each one several containers. Currently only the containers backed by the cgroups file system are supported by design and, more specifically, Linux Containers (LXC) have been used and thus have been proven to work with this framewok.","title":"High-level diagram"},{"location":"architecture/#microservice-architecture","text":"As previously stated, the design followed to create the architecture of this framework uses several microservices that communicate and exchange information. The following image shows a high-level diagram of the microservices layout: As it can be seen, the microservices can be separated into active and passive ones, with the difference being that the passive ones focus on feedback operations to keep the framework continuously updates on the infrastructure's state, and the active ones that use such information to change the state (the container's resource limits) accordingly as needed.","title":"Microservice architecture"},{"location":"architecture/#passive-microservices","text":"The passive microservices are needed to continuosly keep updated the central database (State Database) with all the metada that tracks the state of the infrastructure, from the number of containers and its thresholds to the allocated amounts of resources. Some passive microservices are used to create aggregated data for entities such as applications (i.e., representing a group of containers) or to persist temporary information into a persistent database. Structure Snapshoter : Continuously polls the limits of the containers and writes that information into the State Database. Database Snapshoter : Forwards the information temporarily stored on the State Database to a peristent database, thus creating time series. Refeeder Snapshoter : Aggregates and creates new metadata from existing one (e.g., allocated amount of CPU for an application composed of 3 containers)","title":"Passive Microservices"},{"location":"architecture/#active-microservices","text":"The active microservices are the actual ones that perform the scaling of the resources via changing the container resource limits on the underlying cgroups file system through a coordinated chain of events. As described on the \"Scaling policy\" subsection of the \"Use Case\" section, in order to perform a scaling operation, the resource usage has to exceed the upper, or drop below the lower limit (1). After a configured time amount has passed on this state, the Guardian microservice will create a scaling request (2) with the specific amount of resource to be changed. Such request will be picked up and processed by the Scaler (3) and applied accordingly (4). More specific functionalities of the microservices are described next: Guardian : Working with time windows, it matches the real resource usages of the containers, as provided with the external resource monitoring, with the container limits stored on the State Database. As a result of the matches it generates scaling requests. Scaler : Polls the State Database looking for requests to be applied to the containers.","title":"Active Microservices"},{"location":"architecture/#othercommon-microservices-and-databases","text":"Some microservices have an auxiliar role and are used both by active and passive microservices. Orchestrator : Exposes an API that can be used both by humans or scripts to configure the framework. Container Scaler : This microservice has to be deployed on every infrastructure node whose hosted containers are to be scaled. This service is able to read and write the crgoups file system to perform the actual resource limit scalings.","title":"Other/Common Microservices and Databases"},{"location":"architecture/#more-info","text":"If what is described on this documentation webpage does not answer all of your doubts regarding the technical details, or simply is not enough for you and you want the specifics, this framework has been published on a full-length paper on the FGCS journal. If you don't have access to the journal paper, you can still access the preprint on this link .","title":"More info"},{"location":"configuration/","text":"","title":"Configuration"},{"location":"deployment/","text":"The Serverless Container framework can be deployed by cloning its GitHub repo and placing and starting the proper services on the right environments. Such deployment can be divided into different phases, as next described: Containers Serverless Containers supports any container engine and container technology that is backed by the cgroup file system. Specifically, for development the LXD container manager , which deploys Linux Containers (LXC) , has been used. There is no need for any specific configuration regarding the container deployment, nor there is a need to restart any container to begin adjusting its resources automatically. Nevertheless, in order to successfully perform such resource scaling operations, it may be needed to have permissions to write on the affected container's cgroups files. Databases and dependencies In order to work, Serverless Containers needs to have a constant feed of the resources the containers are using, as close to real-time as possible in order to maintain the responsiveness of the scaling. This feature is currently provided by the BDWatchdog framework, which is mainly composed of a time series database (OpenTSBD) and of a resource monitor agent (atop) coupled with a processing pipeline which are able to work inside containers. In addition, this framework has also at its core a JSON document database used as a cache of the system's state, referred to as State Database . Currently, a CouchDB database is used. The installation of a functional CoudhDB database is quite simple and there is no need for any specific configuration to be used by this frameworks. Nonetheless, because the need for a high response of the database operations, the fact that the stored data does not need to be persisted across time and that the required storage size is relatively small (no more than 1 GiB for 30+ containers), it may be desirable to use in-memory storage Microservices All of the microservices deployed by Serverless Containers have been implemented as Python3 programs and thus, can be started by simply launching them with the system's interpreter. In addition, most of them also interact with the State Database so it is advisable that their latency with the latter is small. Other latencies that may be interesting to take into account, as well as a proposed placement, are shown in the next image. Finally, it should also be considered that some of these microservices, due to their inner operations and continuous polling, can show an overhead that although should not be particularly high, it could be noticeable particularly in experimentation testbeds. Because of this, it is advisable to run as many microservices as possible in an dedicated/isolated instance separated from any 'to-be-kept-clean' environment. Passive services: Structure Snapshoter, Database Snapshoter, Refeeder Snapshoter Regarding the passive microservices, the Structure Snapshoter in particular has to poll the containers via the Container Scaler service, which is deployed in all the infrastructure nodes that host containers. Because of this the lattency of this microservice should also be small when interacting with the nodes. Active services: Guardian, Scaler As with the Structure Snapshoter passive microservice, the Scaler also need to interact with the infrastructure nodes and their Container Scaler service, so the latency between both should be kept low. Other services: Orchestrator, Container Scaler When it comes to the remaining microservices, the Orchestrator should be placed near the State Database , as it may require to perform many database operations in a short amount of time, while the Container Scaler is required to be deployed on each infrastructure node that hosts containers.","title":"Deployment"},{"location":"deployment/#containers","text":"Serverless Containers supports any container engine and container technology that is backed by the cgroup file system. Specifically, for development the LXD container manager , which deploys Linux Containers (LXC) , has been used. There is no need for any specific configuration regarding the container deployment, nor there is a need to restart any container to begin adjusting its resources automatically. Nevertheless, in order to successfully perform such resource scaling operations, it may be needed to have permissions to write on the affected container's cgroups files.","title":"Containers"},{"location":"deployment/#databases-and-dependencies","text":"In order to work, Serverless Containers needs to have a constant feed of the resources the containers are using, as close to real-time as possible in order to maintain the responsiveness of the scaling. This feature is currently provided by the BDWatchdog framework, which is mainly composed of a time series database (OpenTSBD) and of a resource monitor agent (atop) coupled with a processing pipeline which are able to work inside containers. In addition, this framework has also at its core a JSON document database used as a cache of the system's state, referred to as State Database . Currently, a CouchDB database is used. The installation of a functional CoudhDB database is quite simple and there is no need for any specific configuration to be used by this frameworks. Nonetheless, because the need for a high response of the database operations, the fact that the stored data does not need to be persisted across time and that the required storage size is relatively small (no more than 1 GiB for 30+ containers), it may be desirable to use in-memory storage","title":"Databases and dependencies"},{"location":"deployment/#microservices","text":"All of the microservices deployed by Serverless Containers have been implemented as Python3 programs and thus, can be started by simply launching them with the system's interpreter. In addition, most of them also interact with the State Database so it is advisable that their latency with the latter is small. Other latencies that may be interesting to take into account, as well as a proposed placement, are shown in the next image. Finally, it should also be considered that some of these microservices, due to their inner operations and continuous polling, can show an overhead that although should not be particularly high, it could be noticeable particularly in experimentation testbeds. Because of this, it is advisable to run as many microservices as possible in an dedicated/isolated instance separated from any 'to-be-kept-clean' environment.","title":"Microservices"},{"location":"deployment/#passive","text":"services: Structure Snapshoter, Database Snapshoter, Refeeder Snapshoter Regarding the passive microservices, the Structure Snapshoter in particular has to poll the containers via the Container Scaler service, which is deployed in all the infrastructure nodes that host containers. Because of this the lattency of this microservice should also be small when interacting with the nodes.","title":"Passive"},{"location":"deployment/#active","text":"services: Guardian, Scaler As with the Structure Snapshoter passive microservice, the Scaler also need to interact with the infrastructure nodes and their Container Scaler service, so the latency between both should be kept low.","title":"Active"},{"location":"deployment/#other","text":"services: Orchestrator, Container Scaler When it comes to the remaining microservices, the Orchestrator should be placed near the State Database , as it may require to perform many database operations in a short amount of time, while the Container Scaler is required to be deployed on each infrastructure node that hosts containers.","title":"Other"},{"location":"sponsors/","text":"Sponsors The Serverless Containers framework has been developed as part of a PhD thesis from the candidate Jonatan Enes . This frameworks uses another in-house made framework, BDWatchdog , for the resource monitoring requirement, and is in turn used for a specific experimentation scenario for the energy control and capping of containers ( more info ). This PhD thesis is currently being carried out at Universidade da Coru\u00f1a (Spain), in the Computer Architecture group of the Computer Engineering department. Finally, this work has also been possible thanks to the collaboration and funding of several organizations as next presented:","title":"Sponsors"},{"location":"sponsors/#sponsors","text":"The Serverless Containers framework has been developed as part of a PhD thesis from the candidate Jonatan Enes . This frameworks uses another in-house made framework, BDWatchdog , for the resource monitoring requirement, and is in turn used for a specific experimentation scenario for the energy control and capping of containers ( more info ). This PhD thesis is currently being carried out at Universidade da Coru\u00f1a (Spain), in the Computer Architecture group of the Computer Engineering department. Finally, this work has also been possible thanks to the collaboration and funding of several organizations as next presented:","title":"Sponsors"},{"location":"usage/","text":"","title":"Usage"},{"location":"use_case/","text":"This framework is used to scale the resources of a container , or a group of containers, both dynamically and in real time , so that the limits placed on such resources evolve to be just above the usage. On the one hand, from a traditional virtual machine and Cloud perspective, this approach is similar to the on-demand and pay per usage resource provisioning with the main difference being that on this case, the limits can be changed multiple times during execution instead of being specified only once at the instantiation phase. On the other hand, this approach is also really close to the serverless paradigm as the container and all of the processes running on it can not trust the pool of resources exposed to them. Such resources limits can vary according to their usage (e.g., if the CPU usage increases, the CPU limit should also be raised via adding more cores). Combining both of these approaches, this framework comes up with a solution so that virtual infrastructure units, such as software containers (e.g., LXC, Docker), can benefit from having a resource management that implements a serverless scenario. Among other perks, the main benefits of this framework include: Higher resource efficiency , if the containers have a low resource usage, they are given a smaller set of resources, if they have a higher usage, they are given a larger set over time. Pay per 'real' usage billing policy , only the used resources are considered when billing. Flexibility of the containers , which are virtually highly similar to virtual machines and, thus, can be used to deploy a wide arrange of applications. Main Goal The main goal of this framework is to adjust the resource limit, that is, the amount of a resource that a container is given, so that such limit is not far away of the real resource usage. If such limit is way higher than the usage, we can talk of an underutilized resource scenario, while if the limit is close to the usage, there is a bottleneck . The framework makes a best-effort to keep a balance between both scenarios, as if the allocated amount of resource is set too close to the usage, any unexpected increase will face a bottleneck, while if set too high, all of the unused amount is lost considering that in the serverless paradigm only the used resources are billed. Scaling policy In order to better see how the Serverless Containers framework achieves it goal, we can study an example of several scaling operations taking place on a time window. First of all, in the image it can be appreciated that there are: A time series that varies ( orange ), this time series represents the container aggregated resource usage , in this case of CPU. Three varying thresholds (dashed lines), which, from top to bottom, represent the allocated resource limit*+ and the upper and lower boundaries** (more on this later). Three colored areas (blue, green and ochre), which represent the areas between the previously mentioned thresholds. Two vertical lines that do not vary, which represent the maximum and minimum resource limits. As previously stated, the framework looks for a balance when it comes to setting an appropriate allocated resource amount, continuously responding to the resource usage variations. Such response can be seen in the several scaling operations that take place, such as at seconds 70 (down), 270 (up) and 420 (down). In order to detect the conditions that trigger these scaling requirements, two thresholds, or boundaries, are used: the upper boundary , which defines a threshold that, once surpassed, signals for a need to scale up the allocated resource limit to avoid any future bottleneck. the lower boundary , which triggers a scale down of the allocated resource amount once the usage falls beneath the boundary. Thus, it is easy to see that if the thresholds are considered, the first and third scaling operations were caused because the resource usage fell under the lower boundary, while the second was caused because it surpassed the upper boundary. Resource utilization It is interesting to note how important it is to keep a balance that does not impose a bottleneck but also stays close to the real resource usage. As previously stated, the serverless paradigm differs from the Cloud IaaS paradigm in that the resource limits can be modified multiple times over time, instead of just defining such limit once at the startup. Moreover, if we consider that the user of a serverless platform typically does not specify such resource requirements and that the billed resources are only the used ones, the task of properly scaling and setting limits becomes a crucial one which falls to the provider. Because of these reasons it is important to define a key ratio, the resource utilization , which can be easily obtained from the amount of used and the allocated resources. The next image shows the previously used time window but with areas as the focus of the study: We can see that there are three areas: The used area (dots), which represents the amount of resources used by the container. The allocated area (waves), representing the changing amount of resources set apart for this container via the framework and a series of scaling operations. The reserved area (stripes), which represents the theoretical limit of resources that the container had at any moment. It is worth noting that this area would effectively represent the resource footprint of a virtual machine. With these areas it is easy to see that the ratio of utilization of this serverless framework would be higher than the one achieved by a virtual machine. Moreover, an ideal serverless platform, which allocates only the strictly needed resources at any moment, performing instantaneous scaling operation, would have a ratio of 100% (best-case scenario), while the ratio exposed by not performing any scaling operation, such as with the virtual machine, would be the worst-case scenario. Configuration It is worth to be mentioned that the framework is highly configurable, including but not limited to: the time it takes before a scaling operation is triggered the amount of resource that is increased in scaling up operations how wide are the ranges between boundaries Some configuration parameters play a key role in how the framework behaves, nonetheless it would be tiresome for the reader to include all of the details on this webpage, even more considering that they have been extensively referenced and described on the full-length paper ( preprint also publicly available).","title":"Use case"},{"location":"use_case/#main-goal","text":"The main goal of this framework is to adjust the resource limit, that is, the amount of a resource that a container is given, so that such limit is not far away of the real resource usage. If such limit is way higher than the usage, we can talk of an underutilized resource scenario, while if the limit is close to the usage, there is a bottleneck . The framework makes a best-effort to keep a balance between both scenarios, as if the allocated amount of resource is set too close to the usage, any unexpected increase will face a bottleneck, while if set too high, all of the unused amount is lost considering that in the serverless paradigm only the used resources are billed.","title":"Main Goal"},{"location":"use_case/#scaling-policy","text":"In order to better see how the Serverless Containers framework achieves it goal, we can study an example of several scaling operations taking place on a time window. First of all, in the image it can be appreciated that there are: A time series that varies ( orange ), this time series represents the container aggregated resource usage , in this case of CPU. Three varying thresholds (dashed lines), which, from top to bottom, represent the allocated resource limit*+ and the upper and lower boundaries** (more on this later). Three colored areas (blue, green and ochre), which represent the areas between the previously mentioned thresholds. Two vertical lines that do not vary, which represent the maximum and minimum resource limits. As previously stated, the framework looks for a balance when it comes to setting an appropriate allocated resource amount, continuously responding to the resource usage variations. Such response can be seen in the several scaling operations that take place, such as at seconds 70 (down), 270 (up) and 420 (down). In order to detect the conditions that trigger these scaling requirements, two thresholds, or boundaries, are used: the upper boundary , which defines a threshold that, once surpassed, signals for a need to scale up the allocated resource limit to avoid any future bottleneck. the lower boundary , which triggers a scale down of the allocated resource amount once the usage falls beneath the boundary. Thus, it is easy to see that if the thresholds are considered, the first and third scaling operations were caused because the resource usage fell under the lower boundary, while the second was caused because it surpassed the upper boundary.","title":"Scaling policy"},{"location":"use_case/#resource-utilization","text":"It is interesting to note how important it is to keep a balance that does not impose a bottleneck but also stays close to the real resource usage. As previously stated, the serverless paradigm differs from the Cloud IaaS paradigm in that the resource limits can be modified multiple times over time, instead of just defining such limit once at the startup. Moreover, if we consider that the user of a serverless platform typically does not specify such resource requirements and that the billed resources are only the used ones, the task of properly scaling and setting limits becomes a crucial one which falls to the provider. Because of these reasons it is important to define a key ratio, the resource utilization , which can be easily obtained from the amount of used and the allocated resources. The next image shows the previously used time window but with areas as the focus of the study: We can see that there are three areas: The used area (dots), which represents the amount of resources used by the container. The allocated area (waves), representing the changing amount of resources set apart for this container via the framework and a series of scaling operations. The reserved area (stripes), which represents the theoretical limit of resources that the container had at any moment. It is worth noting that this area would effectively represent the resource footprint of a virtual machine. With these areas it is easy to see that the ratio of utilization of this serverless framework would be higher than the one achieved by a virtual machine. Moreover, an ideal serverless platform, which allocates only the strictly needed resources at any moment, performing instantaneous scaling operation, would have a ratio of 100% (best-case scenario), while the ratio exposed by not performing any scaling operation, such as with the virtual machine, would be the worst-case scenario.","title":"Resource utilization"},{"location":"use_case/#configuration","text":"It is worth to be mentioned that the framework is highly configurable, including but not limited to: the time it takes before a scaling operation is triggered the amount of resource that is increased in scaling up operations how wide are the ranges between boundaries Some configuration parameters play a key role in how the framework behaves, nonetheless it would be tiresome for the reader to include all of the details on this webpage, even more considering that they have been extensively referenced and described on the full-length paper ( preprint also publicly available).","title":"Configuration"}]}