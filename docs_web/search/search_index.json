{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This framework, Serverless Containers, is able to dynamically scale a container resource limit ( e.g., CPU, Memory, disk and network ) in order to adapt them to the real usage , at any moment and in real-time. For a brief summary of this tool you can visit its homepage website. In order to see data from real experiments where this tool was used, you can visit this demo . For the uncommented source code you can visit its GitHub . Serverless Containers has also been the subject of a publication in Future Generation Computer Systems (FGCS) , which is available online . In this publication the framework is thoroughly described with all technichal detail, and several experiment examples are also presented. This documentation webpage gives a more detailed description of the framework but without delving into technichal details as in the available publication. The webpage has been structured with the following sections: Use case : This section summarizes the core use case of this framework with a simple example. Architecture : Which briefly describes the architecture and design used. Deployment : In this section it is described how to deploy the framework overall. Some guidelines are also provided. Source Code (external) : If you are interested on the low-level code and code documentation. Sponsors : Some comments on the backers and sponsors of this framework.","title":"Introduction"},{"location":"architecture/","text":"High-level diagram This framework has been designed using a microservice approach in order to ease its development as well as to create speciliazed units that can be reused or improved in isolation. In addition, by using this paradigm it is also possible to implement framework that inherently presents an internal parallelism that is useful when dealing with scenarios that require responsiveness, such as is the case with real-time and on-demand resource scaling. The next image shows a high-level diagram of the scenario on which the framework is deployed: [1] Beginning with the framework's inputs , there are two: 1) the actions, both performed by an user or by another program through the API, that control the framework's behavior; and, 2) the resource monitoring time series, currently provided by an external framework ( BDWatchdog ), that are used in the policy decision for the resource scaling operations. [2] Continuing with this serverlesss containers framework , which groups several microservices, some of which are placed on the controlled hosts. The framework's inner workings are further specified on the following sections. [3] And finishing with the controlled infrastructure , which usually consists of several hosts running each one several instances of containers. Currently only the containers backed by the cgroups file system are supported by design and, more specifically, Linux Containers (LXC) have been used and thus tested to work with this framewok. Design As previously stated, the design followed to create the architecture of this framework uses several microservices that communicate and exchange information. The following image shows a high-level diagram of the microservices layout: When it comes to the microservices Microservices Resource scaling policy More info If what is described on this documentation webpage does not answer all of your doubts regarding the technical details, or simply is not enough for you and you want the specifics, this framework has been published on a full-length paper on the FGCS journal. If you don't have access to the journal paper, you can still access the preprint on this link .","title":"Architecture"},{"location":"architecture/#high-level-diagram","text":"This framework has been designed using a microservice approach in order to ease its development as well as to create speciliazed units that can be reused or improved in isolation. In addition, by using this paradigm it is also possible to implement framework that inherently presents an internal parallelism that is useful when dealing with scenarios that require responsiveness, such as is the case with real-time and on-demand resource scaling. The next image shows a high-level diagram of the scenario on which the framework is deployed: [1] Beginning with the framework's inputs , there are two: 1) the actions, both performed by an user or by another program through the API, that control the framework's behavior; and, 2) the resource monitoring time series, currently provided by an external framework ( BDWatchdog ), that are used in the policy decision for the resource scaling operations. [2] Continuing with this serverlesss containers framework , which groups several microservices, some of which are placed on the controlled hosts. The framework's inner workings are further specified on the following sections. [3] And finishing with the controlled infrastructure , which usually consists of several hosts running each one several instances of containers. Currently only the containers backed by the cgroups file system are supported by design and, more specifically, Linux Containers (LXC) have been used and thus tested to work with this framewok.","title":"High-level diagram"},{"location":"architecture/#design","text":"As previously stated, the design followed to create the architecture of this framework uses several microservices that communicate and exchange information. The following image shows a high-level diagram of the microservices layout: When it comes to the microservices","title":"Design"},{"location":"architecture/#microservices","text":"","title":"Microservices"},{"location":"architecture/#resource-scaling-policy","text":"","title":"Resource scaling policy"},{"location":"architecture/#more-info","text":"If what is described on this documentation webpage does not answer all of your doubts regarding the technical details, or simply is not enough for you and you want the specifics, this framework has been published on a full-length paper on the FGCS journal. If you don't have access to the journal paper, you can still access the preprint on this link .","title":"More info"},{"location":"deployment/","text":"","title":"Deployment"},{"location":"sponsors/","text":"Sponsors The Serverless Containers framework has been developed as part of a PhD thesis from the candidate Jonatan Enes . This frameworks uses another in-house made framework, BDWatchdog , for the resource monitoring requirement, and is in turn used for a specific experimentation scenario for the energy control and capping of containers ( more info ). This PhD thesis is currently being carried out at Universidade da Coru\u00f1a (Spain), in the Computer Architecture group of the Computer Engineering department. Finally, this work has also been possible thanks to the collaboration and funding of several organizations as next presented:","title":"Sponsors"},{"location":"sponsors/#sponsors","text":"The Serverless Containers framework has been developed as part of a PhD thesis from the candidate Jonatan Enes . This frameworks uses another in-house made framework, BDWatchdog , for the resource monitoring requirement, and is in turn used for a specific experimentation scenario for the energy control and capping of containers ( more info ). This PhD thesis is currently being carried out at Universidade da Coru\u00f1a (Spain), in the Computer Architecture group of the Computer Engineering department. Finally, this work has also been possible thanks to the collaboration and funding of several organizations as next presented:","title":"Sponsors"},{"location":"use_case/","text":"This framework is used to scale the resources of a container , or a group of containers, both dynamically and in real time , so that the limits placed on such resources evolve to be just above the usage. On the one hand, from a traditional virtual machine and Cloud perspective, this approach is similar to the on-demand and pay per usage resource provisioning. The main difference would be that on this case, the limits can be changed multiple times during execution instead of being specified only once at the instantiation phase. On the other hand, this approach is also close to the serverless paradigm on the fact that the container, and all of the processes running on it, can not trust the pool of resources exposed to them as such resources limits can vary according to their usage (e.g., if the CPU usage increases, the CPU limit should also be raised via adding more cores). Combining both of these approaches, this framework comes up with a solution so that virtual infrastructure units, such as software containers (e.g., LXC, Docker), can benefit from having a resource management that implements a serverless scenario. Among other perks, the main benefits of this framework include: A higher resource efficiency, those containers with a low resource usage will also be given a smaller set of resources, while those with a higher usage, will have a larger set of resources available over time. Pay per usage billing policy, in the same way as the Cloud platforms, only the used resources should be considered for billing. The flexibility of supporting and using containers, which are virtually highly similar to virtual machines and, thus, can be used to deploy a wide arrange of applications. Main Goal The main goal of this framework is to adjust the resource limit, that is, the amount of a resource that a container is given, so that such limit is not far away of the real resource usage. If such limit is way higher than the usage, we can talk of an underutilized resource scenario, while if the limit is close to the usage, there is a bottleneck . The framework makes a best-effort to keep a balance between both scenarios, as if the allocated amount of resource is set too close to the usage, any unexpected increase will face a bottleneck, while if set too high, all of the unused amount is lost considering that in the serverless paradigm only the used resources are billed. Scaling policy In order o better see how the Serverless Containers framework achieves it goal, we can study an example of several scaling operations taking place on a time window. First of all, in the image it can be appreciated that there are: A time series that varies (orange), this time series represents the container aggregated resource usage, in this case of CPU. Three varying thresholds (dashed lines), which, from top to bottom, represent the allocated resource limit and the upper and lower boundaries (more on this later). Three colored areas (blue, green and ochre), which respresent the areas between the previously mentioned thresholds. Two vertical lines that do not vary, which represent the maximum and minimum resource limits. As previously stated, the framework looks for a balance when it comes to setting an appropriate allocated resource amount, continuously responding to the resource usage variations. Such response can be seen in the several scaling operations that take place, such as at seconds 70 (down), 270 (up) and 420 (down). In order to detect the conditions that trigger these scaling requirements, two thresholds, or boundaries, are used: the upper boundary , which defines a threshold that, once surpassed, signals for a need to scale up the allocated resource limit to avoid any future bottleneck. the lower boundary , which triggers a scale down of the allocated resource amount once the usage falls beneath the boundary. Thus, it is easy to see that if the thresholds are considered, the first and third scaling operations were caused because the resource usage fell under the lower boundary, while the second was caused because it surpassed the upper boundary. Resource utilization It is interesting to note how important it is to keep a balance that does not impose a bottleneck but also stays close to the real resource usage. As previously stated, the serverless paradigm differs from the Cloud IaaS paradigm in that the resource limits can be modified multiple times over time, instead of just defining such limit once at the startup. Moreover, if we consider that the user of a serverless platform typically does not specify such resource requirements, and in addition the billed resources are only the used ones, the task of properly scaling and setting limits becomes a crucial one which falls to the provider. Because of these reasons it is important to define a key ratio, the resource utilization, which can be easily obtained from the amount of used and the allocated resources. The next image shows the same, previously used time window but with the areas as the focus of the study: We can see that there are three areas: The used area (dots), which represents the amount of resources used by the container. The allocated area (waves), representing the changing amount of resources set apart for this container via the framework and a series of scaling operations. The reserved area (stripes), which represents the theoretical limit of resources that the container had at any moment. It is worth noting that this area would effectively represent the resource footprint of a virtual machine. With these areas it is easy to see that the ratio of utilization of this serverless framework would be higher than the one achieved by a virtual machine. Moreover, an ideal serverless platform, which allocates only the strictly needed resources at any moment, performing instantaneous scaling operation, would have a ratio of 100% (best-case scenario), while the ratio exposed by not performing any scaling operation, such as with the virtual machine, would be the worst-case scenario. Configuration It is worth to be mentioned that the framework is highly configurable, including but not limited to: the time it takes before a scaling operation is triggered the amount of resource that is increased in scaling up operations how wide are the ranges between boundaries Some configuration parameters play a key role in how the framework behaves, nonetheless it would be tiresome for the author and the reader to include all of the details on this webpage, even more considering that they have been extensively referenced and described on the full-length paper ( preprint also publicly available).","title":"Use case"},{"location":"use_case/#main-goal","text":"The main goal of this framework is to adjust the resource limit, that is, the amount of a resource that a container is given, so that such limit is not far away of the real resource usage. If such limit is way higher than the usage, we can talk of an underutilized resource scenario, while if the limit is close to the usage, there is a bottleneck . The framework makes a best-effort to keep a balance between both scenarios, as if the allocated amount of resource is set too close to the usage, any unexpected increase will face a bottleneck, while if set too high, all of the unused amount is lost considering that in the serverless paradigm only the used resources are billed.","title":"Main Goal"},{"location":"use_case/#scaling-policy","text":"In order o better see how the Serverless Containers framework achieves it goal, we can study an example of several scaling operations taking place on a time window. First of all, in the image it can be appreciated that there are: A time series that varies (orange), this time series represents the container aggregated resource usage, in this case of CPU. Three varying thresholds (dashed lines), which, from top to bottom, represent the allocated resource limit and the upper and lower boundaries (more on this later). Three colored areas (blue, green and ochre), which respresent the areas between the previously mentioned thresholds. Two vertical lines that do not vary, which represent the maximum and minimum resource limits. As previously stated, the framework looks for a balance when it comes to setting an appropriate allocated resource amount, continuously responding to the resource usage variations. Such response can be seen in the several scaling operations that take place, such as at seconds 70 (down), 270 (up) and 420 (down). In order to detect the conditions that trigger these scaling requirements, two thresholds, or boundaries, are used: the upper boundary , which defines a threshold that, once surpassed, signals for a need to scale up the allocated resource limit to avoid any future bottleneck. the lower boundary , which triggers a scale down of the allocated resource amount once the usage falls beneath the boundary. Thus, it is easy to see that if the thresholds are considered, the first and third scaling operations were caused because the resource usage fell under the lower boundary, while the second was caused because it surpassed the upper boundary.","title":"Scaling policy"},{"location":"use_case/#resource-utilization","text":"It is interesting to note how important it is to keep a balance that does not impose a bottleneck but also stays close to the real resource usage. As previously stated, the serverless paradigm differs from the Cloud IaaS paradigm in that the resource limits can be modified multiple times over time, instead of just defining such limit once at the startup. Moreover, if we consider that the user of a serverless platform typically does not specify such resource requirements, and in addition the billed resources are only the used ones, the task of properly scaling and setting limits becomes a crucial one which falls to the provider. Because of these reasons it is important to define a key ratio, the resource utilization, which can be easily obtained from the amount of used and the allocated resources. The next image shows the same, previously used time window but with the areas as the focus of the study: We can see that there are three areas: The used area (dots), which represents the amount of resources used by the container. The allocated area (waves), representing the changing amount of resources set apart for this container via the framework and a series of scaling operations. The reserved area (stripes), which represents the theoretical limit of resources that the container had at any moment. It is worth noting that this area would effectively represent the resource footprint of a virtual machine. With these areas it is easy to see that the ratio of utilization of this serverless framework would be higher than the one achieved by a virtual machine. Moreover, an ideal serverless platform, which allocates only the strictly needed resources at any moment, performing instantaneous scaling operation, would have a ratio of 100% (best-case scenario), while the ratio exposed by not performing any scaling operation, such as with the virtual machine, would be the worst-case scenario.","title":"Resource utilization"},{"location":"use_case/#configuration","text":"It is worth to be mentioned that the framework is highly configurable, including but not limited to: the time it takes before a scaling operation is triggered the amount of resource that is increased in scaling up operations how wide are the ranges between boundaries Some configuration parameters play a key role in how the framework behaves, nonetheless it would be tiresome for the author and the reader to include all of the details on this webpage, even more considering that they have been extensively referenced and described on the full-length paper ( preprint also publicly available).","title":"Configuration"}]}